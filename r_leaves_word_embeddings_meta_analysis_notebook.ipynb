{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "r/leaves word embeddings meta analysis notebook",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AHqdHpSrQlZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a4cc3954-147b-472f-9e92-618a5f71c986"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from operator import itemgetter\n",
        "from scipy import spatial\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "import inflect\n",
        "import time\n",
        "import numpy as np\n",
        "import statistics\n",
        "import json\n",
        "import itertools\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from datetime import datetime\n",
        "import statistics\n",
        "\n",
        "def _calculate_centroid(model, wordlist):\n",
        "    '''\n",
        "    Calculate centroid of the wordlist list of words based on the model embedding vectors\n",
        "    '''\n",
        "    centr = np.zeros( len(model.wv[wordlist[0]]) )\n",
        "    for w in wordlist:\n",
        "        centr += np.array(model.wv[w])\n",
        "    return centr/len(wordlist)\n",
        "\n",
        "def _keep_only_model_words(model, words):\n",
        "    aux = [ word for word in words if word in model.wv.vocab.keys()]\n",
        "    return aux\n",
        "\n",
        "def _get_word_freq(model, word):\n",
        "    if word in model.wv.vocab:\n",
        "        wm = model.wv.vocab[word]\n",
        "        return [word, wm.count, wm.index]\n",
        "    return None\n",
        "\n",
        "def _get_model_min_max_rank(model):\n",
        "    minF = 999999\n",
        "    maxF = -1\n",
        "    for w in model.wv.vocab:\n",
        "        wm = model.wv.vocab[w] #wm.count, wm.index\n",
        "        rank = wm.index\n",
        "        if(minF>rank):\n",
        "            minF = rank\n",
        "        if(maxF<rank):\n",
        "            maxF = rank\n",
        "    return [minF, maxF]\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "def _get_sentiment(word):\n",
        "    return sid.polarity_scores(word)['compound']\n",
        "\n",
        "'''\n",
        "Normalises a value in the positive space\n",
        "'''    \n",
        "def _normalise(val, minF, maxF):\n",
        "    #print(val, minF, maxF)\n",
        "    if(maxF<0 or minF<0 or val<0):\n",
        "        raise Exception('All values should be in the positive space. minf: {}, max: {}, freq: {}'.format(minF, maxF, val))\n",
        "    if(maxF<= minF):\n",
        "        raise Exception('Maximum frequency should be bigger than min frequency. minf: {}, max: {}, freq: {}'.format(minF, maxF, freq))\n",
        "    val -= minF\n",
        "    val = val/(maxF-minF)\n",
        "    return val\n",
        "\n",
        "def _get_cosine_distance(wv1, wv2):\n",
        "    return spatial.distance.cosine(wv1, wv2)\n",
        "\n",
        "def _get_min_max(dict_value):\n",
        "    l = list(dict_value.values())\n",
        "    return [ min(l), max(l)]\n",
        "\n",
        "def _find_stdev_threshold_sal(dwords, stdevs):\n",
        "    '''\n",
        "    dword is an object like {'word':w, 'bias':bias, 'biasW':biasW, 'freq':freq, 'freqW':freqW, 'sal':val, 'wv':wv, 'sent':sent }\n",
        "    stdevs : minimum stdevs for which we want to compute the threshold\n",
        "\n",
        "    returns\n",
        "    outlier_thr : the threshold correpsonding to stdevs considering salience values from the dwrods object list\n",
        "    '''\n",
        "    allsal = []\n",
        "    for obj in dwords:\n",
        "        allsal.append(obj['sal'])\n",
        "    stdev = statistics.stdev(allsal)\n",
        "    outlier_thr = (stdev*stdevs)+sum(allsal)/len(allsal)\n",
        "    return outlier_thr\n",
        "\n",
        "def calculate_biased_words(model, targetset1, targetset2, stdevs, \n",
        "                         acceptedPOS = ['JJ', 'JJS', 'JJR','NN', 'NNS', 'NNP', 'NNPS','VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ' ], \n",
        "                         words = None, force=False):\n",
        "    '''\n",
        "    this function calculates the list of biased words towards targetset1 and taregset2 with salience > than the \n",
        "    specified times (minstdev) of standard deviation.\n",
        "\n",
        "    targetset1 <list of strings> : target set 1\n",
        "    targetset2 <list of strings> : target set 2\n",
        "    minstdev int : Minium threhsold for stdev to select biased words\n",
        "    acceptedPOS <list<str>> : accepted list of POS to consider for the analysis, as defined in NLTK POS tagging lib. \n",
        "                              If None, no POS filtering is applied and all words in the vocab are considered\n",
        "    words list<str> : list of words we want to consider. If None, all words in the vocab are considered\n",
        "    '''\n",
        "    if(model is None):\n",
        "        raise Exception(\"You need to define a model to estimate biased words.\")\n",
        "    if(targetset1 is None or targetset2 is None):\n",
        "        raise Exception(\"Target sets are necessary to estimate biased words.\")\n",
        "    if(stdevs is None):\n",
        "        raise Exception(\"You need to define a minimum threshold for standard deviation to select biased words.\")\n",
        "   \n",
        "    tset1 = _keep_only_model_words(model, targetset1) # remove target set words that do not exist in the model\n",
        "    tset2 = _keep_only_model_words(model, targetset2) # remove target set words that do not exist in the model\n",
        "\n",
        "    # We remove words in the target sets, and also their plurals from the set of interesting words to process.\n",
        "    engine = inflect.engine()\n",
        "    toremove = targetset1 + targetset2 + [engine.plural(w) for w in targetset1] + [engine.plural(w) for w in targetset2]\n",
        "    if(words is None):\n",
        "        words = [w for w in model.wv.vocab.keys() if w not in toremove]\n",
        "\n",
        "    # Calculate centroids \n",
        "    tset1_centroid = _calculate_centroid(model, tset1)\n",
        "    tset2_centroid = _calculate_centroid(model, tset2)\n",
        "    [minR, maxR] = _get_model_min_max_rank(model)\n",
        "\n",
        "    # Get biases for words\n",
        "    biasWF = {}\n",
        "    biasWM = {}\n",
        "    for i, w in enumerate(words):\n",
        "        p = nltk.pos_tag([w])[0][1]\n",
        "        if acceptedPOS is not None and p not in acceptedPOS:\n",
        "            continue\n",
        "        wv = model.wv[w]\n",
        "        diff = _get_cosine_distance(tset2_centroid, wv) - _get_cosine_distance(tset1_centroid, wv)\n",
        "        if(diff>0):\n",
        "            biasWF[w] = diff\n",
        "        else:\n",
        "            biasWM[w] = -1*diff\n",
        "\n",
        "    # Get min and max bias for both target sets, so we can normalise these values later\n",
        "    [minbf, maxbf] = _get_min_max(biasWF)\n",
        "    [minbm, maxbm] = _get_min_max(biasWM)\n",
        "\n",
        "    # Iterate through all 'selected' words\n",
        "    biased1 = []\n",
        "    biased2 = []\n",
        "    for i, w in enumerate(words):\n",
        "        # Print('..Processing ', w)\n",
        "        p = nltk.pos_tag([w])[0][1]\n",
        "        if acceptedPOS is not None and p not in acceptedPOS:\n",
        "            continue\n",
        "        wv = model.wv[w]\n",
        "        # Sentiment\n",
        "        sent = _get_sentiment(w)\n",
        "        # Rank and rank norm\n",
        "        freq = _get_word_freq(model, w)[1]\n",
        "        rank = _get_word_freq(model, w)[2]\n",
        "        rankW = 1-_normalise(rank, minR, maxR) \n",
        "\n",
        "        # Normalise bias\n",
        "        if(w in biasWF):\n",
        "            bias = biasWF[w]\n",
        "            biasW = _normalise(bias, minbf, maxbf)\n",
        "            val = biasW * rankW\n",
        "            biased1.append({'word':w, 'bias':bias, 'biasW':biasW, 'freq':freq, 'rank':rank, 'rankW':rankW, 'sal':val, 'wv':wv.tolist(), 'sent':sent } ) \n",
        "        if(w in biasWM):\n",
        "            bias = biasWM[w]\n",
        "            biasW = _normalise(bias, minbm, maxbm)\n",
        "            val = biasW * rankW\n",
        "            biased2.append({'word':w, 'bias':bias, 'biasW':biasW, 'freq':freq, 'rank':rank, 'rankW':rankW, 'sal':val, 'wv':wv.tolist(), 'sent':sent } ) \n",
        "\n",
        "    # Calculate the salience threshold for both word sets, and select the list of biased words (i.e., which words do we discard?)\n",
        "    stdevs1_thr = _find_stdev_threshold_sal(biased1, stdevs)\n",
        "    stdevs2_thr = _find_stdev_threshold_sal(biased2, stdevs)\n",
        "    # biased1.sort(key=lambda x: x['sal'], reverse=True)\n",
        "    b1_dict = {}\n",
        "    for k in biased1:\n",
        "        if(k['sal']>=stdevs1_thr):\n",
        "            b1_dict[k['word']] = k\n",
        "    # biased2.sort(key=lambda x: x['sal'], reverse=True)\n",
        "    b2_dict = {}\n",
        "    for k in biased2:\n",
        "        if(k['sal']>=stdevs2_thr):\n",
        "            b2_dict[k['word']] = k\n",
        "\n",
        "    #transform centroid tol list so they become serializable\n",
        "    tset1_centroid = tset1_centroid.tolist() \n",
        "    tset2_centroid = tset2_centroid.tolist()\n",
        "    return [b1_dict, b2_dict]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0q_ELtv4-TY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "b161e8ff-a209-4cfe-8152-7949dbd9de97"
      },
      "source": [
        "\n",
        "modelpath = \"leaves_w4_f10_e100_d150.model\"\n",
        "model = Word2Vec.load(modelpath)\n",
        "\n",
        "import time\n",
        "starttime = time.time()\n",
        "\n",
        "\n",
        "progress = ['discussion', 'find', 'understanding', 'useful', 'support', 'group', 'approach', 'subreddit', 'perspective', 'benefit', 'therapy', 'positive', 'improve', 'helpful', 'reflect', 'motivate', 'practice', 'sub', 'focused', 'provide', 'information', 'knowledge', 'encourage', 'meditate', 'focusing', 'mindfulness', 'guidance', 'outlook', 'skill', 'guide', 'headspace', 'technique', 'forum', 'compassion', 'practicing', 'tool', 'insight', 'techniques', 'achieve', 'accountability', 'awareness', 'meditating', 'buddhism', 'channel', 'importance', 'identify', 'beneficial', 'engage', 'explore', 'constructive', 'mindful', 'cultivate', 'resources', 'resource', 'acceptance', 'journaling', 'grounded', 'positivity', 'spirituality', 'mediation', 'gratitude', 'refocus', 'community', 'help', 'exercise', 'success', 'create', 'recovery', 'others', 'address', 'methods', 'learn', 'interests', 'journey', 'seek', 'practical', 'strategies', 'reflection', 'meditation', 'develop', 'discipline', 'value', 'growth', 'ideas', 'focus', 'strategy', 'tips', 'behavioral', 'solutions', 'wisdom', 'philosophy', 'introspection', 'structure', 'kindness', 'implement', 'info', 'discuss', 'utilize', 'strengthen', 'dbt', 'meditations', 'reinforce', 'cbt', 'stoicism', 'build', 'experiences', 'form', 'overcome', 'ways', 'suggestions', 'activities', 'groups', 'research', 'insights', 'website', 'lessons', 'psychology', 'mechanisms', 'alternatives', 'literature', 'sources', 'principles', 'behaviors', 'tools', 'perspectives', 'practices', 'habits', 'areas', 'network', 'subreddits', 'challenges', 'options', 'evidence', 'program', 'advice', 'hobbies', 'strength', 'exercises', 'aspects', 'boundaries', 'books', 'concepts', 'studies', 'programs', 'link', 'beliefs', 'foundation', 'concept', 'method', 'apps', 'outlets', 'communities', 'topics', 'links', 'routines', 'data', 'qualities', 'values', 'material', 'recommendations', 'people', 'meetings', 'points', 'things', 'medications', 'stories', 'posts', 'websites', 'site', 'folks', 'projects', 'goals', 'addictions', 'path', 'results', 'vices', 'diet', 'ones', 'threads', 'articles', 'subs', 'skills', 'circumstances', 'examples', 'efforts', 'parts', 'opinions', 'rules', 'supplements', 'factors', 'videos', 'distractions', 'conditions', 'forms', 'guidelines', 'therapists', 'traits', 'affirmations', 'friendships', 'context', 'reasons', 'quotes', 'stuff', 'words', 'benefits', 'steps', 'foods', 'book', 'professionals', 'struggles', 'successes', 'places', 'answers', 'types', 'individuals', 'forums', 'questions', 'substances', 'outcomes', 'remedies', 'accounts', 'treatments', 'stats', 'journeys', 'items', 'possibilities', 'courses', 'services', 'elements', 'teachings']\n",
        "exasperation = ['gonna', 'haha', 'coke', 'had', 'yup', 'half', 'yeah', 'bad', 'depressed', 'hell', 'kinda', 'miserable', 'fucked', 'bitch', 'sucked', 'wtf', 'insane', 'ridiculous', 'fuck', 'drunk', 'awful', 'sucks', 'omg', 'ass', 'pathetic', 'nasty', 'tho', 'ha', 'oh', 'terrible', 'yesterday', 'sad', 'resin', 'dead', 'retarded', 'sack', 'everytime', 'stupid', 'lol', 'wicked', 'crazy', 'gross', 'crappy', 'fiend', 'sick', 'crack', 'grumpy', 'bc', 'cuz', 'nope', 'dope', 'yea', 'weird', 'horrible', 'lame', 'disgusting', 'dumb', 'balls', 'dank', 'hella', 'eh', 'didnt', 'crap', 'freaking', 'legit', 'yep', 'nah', 'dirty', 'fucker', 'pissed', 'junkie', 'garbage', 'hangovers', 'hahaha', 'dang', 'dying', 'bastard', 'burnt', 'cus', 'worthless', 'hahah', 'lmao', 'crackhead', 'af', 'bruh', 'carts', 'didn', 'last', 'anyways', 'mad', 'pussy', 'alright', 'meh', 'hated', 'lazy', 'straight', 'ok', 'moody', 'paranoid', 'tired', 'anxious', 'lethargic', 'meth', 'irritable', 'sorta', 'dry', 'hungry', 'cos', 'nuts', 'embarrassing', 'cranky', 'annoying', 'broke', 'scared', 'nauseated', 'bum', 'loser', 'tbh', 'guilty', 'till', 'boy', 'groggy', 'idiot', 'brutal', 'nauseous', 'hungover', 'foggy', 'bummed', 'wet', 'dizzy', 'terrified', 'rn', 'lmfao', 'fine', 'okay', 'high', 'stoned', 'afterwards', 'upset', 'blazed', 'weak', 'embarrassed', 'bored', 'forgetful', 'stressed', 'awkward', 'boring', 'empty', 'angry', 'nervous', 'restless', 'ashamed', 'baked', 'depressing', 'exhausted', 'frustrated', 'irritated', 'disappointed', 'confused', 'bitter', 'cloudy', 'tempted', 'panicked', 'zombie', 'hazy', 'dehydrated', 'fatigued', 'uncomfortable', 'annoyed', 'crying', 'edgy', 'sore', 'agitated', 'skinny', 'unmotivated', 'sluggish', 'hopeless', 'drained', 'sweaty', 'horny', 'bloated', 'odd', 'low', 'hate', 'stuck', 'worse', 'sleepy', 'tempting', 'unhappy', 'unproductive', 'worried', 'overwhelmed', 'frustrating', 'dull', 'jealous', 'suicidal', 'desperate', 'strange', 'rough', 'scary', 'fried', 'antsy', 'trapped', 'apathetic', 'useless', 'relieved', 'fuzzy', 'disgusted', 'pointless', 'unstable', 'unbearable', 'thirsty', 'shaky', 'uneasy', 'normal', 'overwhelming', 'stressful', 'numb', 'intense', 'insecure', 'drowsy', 'isolated', 'jittery', 'unpleasant', 'relaxed', 'impatient', 'exhausting', 'defeated', 'disconnected']\n",
        "[cycle1, cycle1a] = calculate_biased_words(model, progress, exasperation, 4)\n",
        "[cycle2, cycle2a] = calculate_biased_words(model, [w for w in cycle1.keys()], [w for w in cycle1a.keys()], 4)\n",
        "[cycle3, cycle3a] = calculate_biased_words(model, [w for w in cycle2.keys()], [w for w in cycle2a.keys()], 4)\n",
        "[cycle4, cycle4a] = calculate_biased_words(model, [w for w in cycle3.keys()], [w for w in cycle3a.keys()], 4)\n",
        "[cycle5, cycle5a] = calculate_biased_words(model, [w for w in cycle4.keys()], [w for w in cycle4a.keys()], 4)\n",
        "[cycle6, cycle6a] = calculate_biased_words(model, [w for w in cycle5.keys()], [w for w in cycle5a.keys()], 4)\n",
        "print('-> meta-analysis only took us {} seconds!'.format(time.time()-starttime))\n",
        "  \n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7e4baa9ce52d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodelpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"leaves_w4_f10_e100_d150.model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \"\"\"\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \"\"\"\n\u001b[0;32m-> 1358\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     return open(uri, mode, ignore_ext=ignore_extension,\n\u001b[0;32m--> 310\u001b[0;31m                 transport_params=transport_params, **scrubbed_kwargs)\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'leaves_w4_f10_e100_d150.model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc-9g8BErQll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "9cf6b6b6-fd48-4d7f-cfb1-ba8ccbd7f610"
      },
      "source": [
        "print('My seed words related to progress:')\n",
        "print(progress)\n",
        "print()\n",
        "print('Biased words towards progress')\n",
        "print( [w for w in cycle1.keys()] )\n",
        "print()\n",
        "print('Layer 1 metabiased words towards progress')\n",
        "print( [w for w in cycle2.keys()])\n",
        "print()\n",
        "print('Layer 3 metabiased words towards progress')\n",
        "print( [w for w in cycle3.keys()])\n",
        "print()\n",
        "print('Layer 4 metabiased words towards progress')\n",
        "print( [w for w in cycle4.keys()])\n",
        "print()\n",
        "print('Layer 4 metabiased words towards progress')\n",
        "print( [w for w in cycle5.keys()])\n",
        "print()\n",
        "print('Layer 5 metabiased words towards progress')\n",
        "print( [w for w in cycle6.keys()])\n",
        "print()\n",
        "print()\n",
        "print('My seed words related to exasperation:')\n",
        "print(exasperation)\n",
        "print()\n",
        "print('Biased words towards exasperation')\n",
        "print( [w for w in cycle1a.keys()] )\n",
        "print()\n",
        "print('Layer 1 metabiased words towards exasperation')\n",
        "print( [w for w in cycle2a.keys()])\n",
        "print()\n",
        "print('Layer 2 metabiased words towards exasperation')\n",
        "print( [w for w in cycle3a.keys()])\n",
        "print()\n",
        "print('Layer 3 metabiased words towards exasperation')\n",
        "print( [w for w in cycle4a.keys()])\n",
        "print()\n",
        "print('Layer 4 metabiased words towards exasperation')\n",
        "print( [w for w in cycle5a.keys()])\n",
        "print()\n",
        "print('Layer 5 metabiased words towards exasperation')\n",
        "print( [w for w in cycle6a.keys()])\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My seed words related to progress:\n",
            "['meditation', 'help', 'mindset', 'clear', 'focus', 'sober', 'suppoRt', 'community', 'love', 'self-love', 'loving']\n",
            "\n",
            "Biased words towards progress\n",
            "['discussion', 'find', 'understanding', 'useful', 'support', 'group', 'approach', 'subreddit', 'perspective', 'benefit', 'therapy', 'positive', 'improve', 'helpful', 'reflect', 'motivate', 'practice', 'sub', 'focused', 'provide', 'information', 'knowledge', 'encourage', 'meditate', 'focusing', 'mindfulness', 'guidance', 'outlook', 'skill', 'guide', 'headspace', 'technique', 'forum', 'compassion', 'practicing', 'tool', 'insight', 'techniques', 'achieve', 'accountability', 'awareness', 'meditating', 'buddhism', 'channel', 'importance', 'identify', 'beneficial', 'engage', 'explore', 'constructive', 'mindful', 'cultivate', 'resources', 'resource', 'acceptance', 'journaling', 'grounded', 'positivity', 'spirituality', 'mediation', 'gratitude', 'refocus']\n",
            "\n",
            "Layer 1 metabiased words towards progress\n",
            "['community', 'help', 'exercise', 'success', 'create', 'recovery', 'others', 'address', 'methods', 'learn', 'interests', 'journey', 'seek', 'practical', 'strategies', 'reflection', 'meditation', 'develop', 'discipline', 'value', 'growth', 'ideas', 'focus', 'strategy', 'tips', 'behavioral', 'solutions', 'wisdom', 'philosophy', 'introspection', 'structure', 'kindness', 'implement', 'info', 'discuss', 'utilize', 'strengthen', 'dbt', 'meditations', 'reinforce', 'cbt', 'stoicism']\n",
            "\n",
            "Layer 3 metabiased words towards progress\n",
            "['discussion', 'find', 'understanding', 'support', 'build', 'group', 'approach', 'experiences', 'form', 'perspective', 'overcome', 'therapy', 'ways', 'suggestions', 'activities', 'groups', 'practice', 'provide', 'research', 'insights', 'website', 'information', 'knowledge', 'mindfulness', 'guidance', 'skill', 'lessons', 'guide', 'psychology', 'mechanisms', 'technique', 'compassion', 'alternatives', 'literature', 'tool', 'sources', 'principles', 'insight', 'techniques', 'behaviors', 'awareness', 'buddhism', 'channel', 'importance', 'tools', 'perspectives', 'engage', 'explore', 'cultivate', 'resources', 'resource', 'acceptance', 'spirituality', 'mediation', 'practices']\n",
            "\n",
            "Layer 4 metabiased words towards progress\n",
            "['community', 'habits', 'success', 'areas', 'network', 'subreddits', 'create', 'recovery', 'challenges', 'others', 'options', 'evidence', 'program', 'methods', 'advice', 'hobbies', 'strength', 'interests', 'journey', 'exercises', 'seek', 'aspects', 'boundaries', 'books', 'strategies', 'concepts', 'meditation', 'studies', 'develop', 'programs', 'discipline', 'value', 'link', 'growth', 'ideas', 'beliefs', 'strategy', 'tips', 'foundation', 'concept', 'behavioral', 'solutions', 'wisdom', 'method', 'philosophy', 'apps', 'outlets', 'communities', 'topics', 'introspection', 'structure', 'links', 'routines', 'info', 'data', 'qualities', 'utilize', 'dbt', 'meditations', 'values', 'material', 'recommendations', 'cbt', 'stoicism']\n",
            "\n",
            "Layer 4 metabiased words towards progress\n",
            "['discussion', 'people', 'meetings', 'points', 'things', 'medications', 'support', 'group', 'approach', 'experiences', 'stories', 'form', 'posts', 'websites', 'site', 'perspective', 'folks', 'projects', 'goals', 'addictions', 'path', 'results', 'ways', 'vices', 'suggestions', 'diet', 'activities', 'ones', 'threads', 'groups', 'practice', 'articles', 'subs', 'research', 'skills', 'circumstances', 'insights', 'examples', 'website', 'information', 'knowledge', 'efforts', 'mindfulness', 'guidance', 'skill', 'lessons', 'guide', 'parts', 'opinions', 'rules', 'psychology', 'mechanisms', 'technique', 'supplements', 'alternatives', 'literature', 'tool', 'sources', 'principles', 'insight', 'techniques', 'factors', 'behaviors', 'buddhism', 'channel', 'videos', 'importance', 'tools', 'distractions', 'perspectives', 'conditions', 'forms', 'resources', 'resource', 'guidelines', 'therapists', 'traits', 'practices', 'affirmations']\n",
            "\n",
            "Layer 5 metabiased words towards progress\n",
            "['community', 'habits', 'areas', 'network', 'subreddits', 'subreddit', 'friendships', 'recovery', 'context', 'challenges', 'others', 'options', 'evidence', 'program', 'reasons', 'quotes', 'methods', 'stuff', 'words', 'benefits', 'advice', 'hobbies', 'steps', 'foods', 'interests', 'journey', 'exercises', 'book', 'aspects', 'boundaries', 'professionals', 'struggles', 'successes', 'books', 'places', 'strategies', 'answers', 'types', 'concepts', 'individuals', 'meditation', 'forums', 'studies', 'programs', 'link', 'ideas', 'questions', 'beliefs', 'strategy', 'tips', 'foundation', 'concept', 'substances', 'solutions', 'wisdom', 'method', 'philosophy', 'apps', 'outlets', 'forum', 'communities', 'topics', 'introspection', 'structure', 'outcomes', 'links', 'routines', 'info', 'remedies', 'data', 'qualities', 'accounts', 'treatments', 'stats', 'meditations', 'values', 'journeys', 'items', 'material', 'possibilities', 'recommendations', 'courses', 'services', 'cbt', 'elements', 'teachings', 'stoicism']\n",
            "\n",
            "\n",
            "My seed words related to exasperation:\n",
            "['frick', 'frickin', 'freakin', 'freaking,\"tweak\", \"tweaking\",\"fuck', 'fuckin', 'fucking', 'fucky', 'fuckedfuckedness', 'motherfucking', 'motherfucker', 'damn', 'goddamn', 'shit', 'shitty', 'shittier', 'shittiest', 'blasted', 'bloody']\n",
            "\n",
            "Biased words towards exasperation\n",
            "['gonna', 'haha', 'coke', 'had', 'yup', 'half', 'yeah', 'bad', 'depressed', 'hell', 'kinda', 'miserable', 'fucked', 'bitch', 'sucked', 'wtf', 'insane', 'ridiculous', 'fuck', 'drunk', 'awful', 'sucks', 'omg', 'ass', 'pathetic', 'nasty', 'tho', 'ha', 'oh', 'terrible', 'yesterday', 'sad', 'resin', 'dead', 'retarded', 'sack', 'everytime', 'stupid', 'lol', 'wicked', 'crazy', 'gross', 'crappy', 'fiend', 'sick', 'crack', 'grumpy', 'bc', 'cuz', 'nope', 'dope', 'yea', 'weird', 'horrible', 'lame', 'disgusting', 'dumb', 'balls', 'dank', 'hella', 'eh', 'didnt', 'crap', 'freaking', 'legit', 'yep', 'nah', 'dirty', 'fucker', 'pissed', 'junkie', 'garbage', 'hangovers', 'hahaha', 'dang', 'dying', 'bastard', 'burnt', 'cus', 'worthless', 'hahah', 'lmao', 'crackhead', 'af', 'bruh', 'carts']\n",
            "\n",
            "Layer 1 metabiased words towards exasperation\n",
            "['didn', 'last', 'shitty', 'anyways', 'mad', 'pussy', 'shit', 'fucking', 'alright', 'meh', 'hated', 'damn', 'lazy', 'straight', 'ok', 'moody', 'paranoid', 'tired', 'anxious', 'lethargic', 'goddamn', 'meth', 'irritable', 'sorta', 'dry', 'hungry', 'cos', 'nuts', 'embarrassing', 'cranky', 'annoying', 'broke', 'fuckin', 'scared', 'nauseated', 'bum', 'loser', 'tbh', 'guilty', 'till', 'boy', 'groggy', 'idiot', 'brutal', 'nauseous', 'hungover', 'foggy', 'bummed', 'wet', 'dizzy', 'terrified', 'rn', 'lmfao', 'bloody']\n",
            "\n",
            "Layer 2 metabiased words towards exasperation\n",
            "['fine', 'yup', 'okay', 'high', 'yeah', 'bad', 'stoned', 'afterwards', 'upset', 'depressed', 'blazed', 'kinda', 'weak', 'embarrassed', 'miserable', 'bored', 'fucked', 'bitch', 'sucked', 'wtf', 'insane', 'forgetful', 'stressed', 'awkward', 'ridiculous', 'drunk', 'awful', 'sucks', 'omg', 'boring', 'pathetic', 'terrible', 'yesterday', 'sad', 'empty', 'dead', 'retarded', 'stupid', 'angry', 'lol', 'nervous', 'crazy', 'gross', 'restless', 'crappy', 'fiend', 'sick', 'ashamed', 'grumpy', 'cuz', 'yea', 'weird', 'horrible', 'lame', 'baked', 'disgusting', 'dumb', 'depressing', 'exhausted', 'frustrated', 'irritated', 'disappointed', 'confused', 'bitter', 'cloudy', 'tempted', 'pissed', 'panicked', 'zombie', 'hazy', 'dehydrated', 'fatigued', 'uncomfortable', 'annoyed', 'crying', 'edgy', 'sore', 'agitated', 'dying', 'skinny', 'unmotivated', 'burnt', 'sluggish', 'hopeless', 'worthless', 'hahah', 'drained', 'lmao', 'sweaty', 'horny', 'bloated', 'af']\n",
            "\n",
            "Layer 3 metabiased words towards exasperation\n",
            "['odd', 'low', 'shitty', 'mad', 'pussy', 'fucking', 'hate', 'stuck', 'worse', 'alright', 'meh', 'sleepy', 'hated', 'lazy', 'ok', 'tempting', 'unhappy', 'unproductive', 'nasty', 'worried', 'overwhelmed', 'moody', 'paranoid', 'frustrating', 'tired', 'anxious', 'lethargic', 'dull', 'jealous', 'irritable', 'suicidal', 'dry', 'hungry', 'embarrassing', 'desperate', 'strange', 'cranky', 'annoying', 'fuckin', 'scared', 'nauseated', 'rough', 'bum', 'scary', 'loser', 'guilty', 'fried', 'groggy', 'antsy', 'trapped', 'apathetic', 'useless', 'nauseous', 'hungover', 'foggy', 'relieved', 'bummed', 'fuzzy', 'disgusted', 'pointless', 'unstable', 'unbearable', 'dizzy', 'terrified', 'thirsty', 'shaky']\n",
            "\n",
            "Layer 4 metabiased words towards exasperation\n",
            "['fine', 'okay', 'high', 'bad', 'stoned', 'upset', 'depressed', 'uneasy', 'normal', 'weak', 'embarrassed', 'miserable', 'bored', 'fucked', 'sucked', 'insane', 'forgetful', 'stressed', 'awkward', 'ridiculous', 'overwhelming', 'stressful', 'drunk', 'awful', 'sucks', 'boring', 'pathetic', 'terrible', 'sad', 'empty', 'retarded', 'stupid', 'angry', 'nervous', 'crazy', 'gross', 'restless', 'crappy', 'sick', 'numb', 'ashamed', 'grumpy', 'weird', 'horrible', 'lame', 'baked', 'disgusting', 'dumb', 'depressing', 'exhausted', 'frustrated', 'irritated', 'intense', 'disappointed', 'confused', 'bitter', 'insecure', 'cloudy', 'tempted', 'pissed', 'panicked', 'zombie', 'hazy', 'dehydrated', 'fatigued', 'uncomfortable', 'annoyed', 'edgy', 'drowsy', 'agitated', 'isolated', 'dying', 'unmotivated', 'burnt', 'sluggish', 'hopeless', 'worthless', 'drained', 'sweaty', 'horny', 'bloated', 'jittery']\n",
            "\n",
            "Layer 5 metabiased words towards exasperation\n",
            "['odd', 'low', 'shitty', 'afterwards', 'mad', 'blazed', 'hate', 'stuck', 'worse', 'alright', 'meh', 'sleepy', 'hated', 'lazy', 'ok', 'tempting', 'unhappy', 'unproductive', 'overwhelmed', 'moody', 'paranoid', 'unpleasant', 'frustrating', 'tired', 'anxious', 'lethargic', 'dull', 'irritable', 'suicidal', 'hungry', 'relaxed', 'embarrassing', 'desperate', 'impatient', 'exhausting', 'strange', 'cranky', 'annoying', 'scared', 'nauseated', 'rough', 'scary', 'guilty', 'fried', 'groggy', 'antsy', 'trapped', 'apathetic', 'useless', 'nauseous', 'hungover', 'foggy', 'defeated', 'sore', 'relieved', 'bummed', 'fuzzy', 'disgusted', 'pointless', 'unstable', 'unbearable', 'dizzy', 'terrified', 'disconnected', 'thirsty', 'shaky']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25-xFZW4lzOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5d131b13-3f56-4d44-b9e7-aa001cc1e143"
      },
      "source": [
        "progwordbank = ['discussion', 'find', 'understanding', 'useful', 'support', 'group', 'approach', 'subreddit', 'perspective', 'benefit', 'therapy', 'positive', 'improve', 'helpful', 'reflect', 'motivate', 'practice', 'sub', 'focused', 'provide', 'information', 'knowledge', 'encourage', 'meditate', 'focusing', 'mindfulness', 'guidance', 'outlook', 'skill', 'guide', 'headspace', 'technique', 'forum', 'compassion', 'practicing', 'tool', 'insight', 'techniques', 'achieve', 'accountability', 'awareness', 'meditating', 'buddhism', 'channel', 'importance', 'identify', 'beneficial', 'engage', 'explore', 'constructive', 'mindful', 'cultivate', 'resources', 'resource', 'acceptance', 'journaling', 'grounded', 'positivity', 'spirituality', 'mediation', 'gratitude', 'refocus','community', 'help', 'exercise', 'success', 'create', 'recovery', 'others', 'address', 'methods', 'learn', 'interests', 'journey', 'seek', 'practical', 'strategies', 'reflection', 'meditation', 'develop', 'discipline', 'value', 'growth', 'ideas', 'focus', 'strategy', 'tips', 'behavioral', 'solutions', 'wisdom', 'philosophy', 'introspection', 'structure', 'kindness', 'implement', 'info', 'discuss', 'utilize', 'strengthen', 'dbt', 'meditations', 'reinforce', 'cbt', 'stoicism', 'discussion', 'find', 'understanding', 'support', 'build', 'group', 'approach', 'experiences', 'form', 'perspective', 'overcome', 'therapy', 'ways', 'suggestions', 'activities', 'groups', 'practice', 'provide', 'research', 'insights', 'website', 'information', 'knowledge', 'mindfulness', 'guidance', 'skill', 'lessons', 'guide', 'psychology', 'mechanisms', 'technique', 'compassion', 'alternatives', 'literature', 'tool', 'sources', 'principles', 'insight', 'techniques', 'behaviors', 'awareness', 'buddhism', 'channel', 'importance', 'tools', 'perspectives', 'engage', 'explore', 'cultivate', 'resources', 'resource', 'acceptance', 'spirituality', 'mediation', 'practices','community', 'habits', 'success', 'areas', 'network', 'subreddits', 'create', 'recovery', 'challenges', 'others', 'options', 'evidence', 'program', 'methods', 'advice', 'hobbies', 'strength', 'interests', 'journey', 'exercises', 'seek', 'aspects', 'boundaries', 'books', 'strategies', 'concepts', 'meditation', 'studies', 'develop', 'programs', 'discipline', 'value', 'link', 'growth', 'ideas', 'beliefs', 'strategy', 'tips', 'foundation', 'concept', 'behavioral', 'solutions', 'wisdom', 'method', 'philosophy', 'apps', 'outlets', 'communities', 'topics', 'introspection', 'structure', 'links', 'routines', 'info', 'data', 'qualities', 'utilize', 'dbt', 'meditations', 'values', 'material', 'recommendations', 'cbt', 'stoicism', 'discussion', 'people', 'meetings', 'points', 'things', 'medications', 'support', 'group', 'approach', 'experiences', 'stories', 'form', 'posts', 'websites', 'site', 'perspective', 'folks', 'projects', 'goals', 'addictions', 'path', 'results', 'ways', 'vices', 'suggestions', 'diet', 'activities', 'ones', 'threads', 'groups', 'practice', 'articles', 'subs', 'research', 'skills', 'circumstances', 'insights', 'examples', 'website', 'information', 'knowledge', 'efforts', 'mindfulness', 'guidance', 'skill', 'lessons', 'guide', 'parts', 'opinions', 'rules', 'psychology', 'mechanisms', 'technique', 'supplements', 'alternatives', 'literature', 'tool', 'sources', 'principles', 'insight', 'techniques', 'factors', 'behaviors', 'buddhism', 'channel', 'videos', 'importance', 'tools', 'distractions', 'perspectives', 'conditions', 'forms', 'resources', 'resource', 'guidelines', 'therapists', 'traits', 'practices', 'affirmations', 'community', 'habits', 'areas', 'network', 'subreddits', 'subreddit', 'friendships', 'recovery', 'context', 'challenges', 'others', 'options', 'evidence', 'program', 'reasons', 'quotes', 'methods', 'stuff', 'words', 'benefits', 'advice', 'hobbies', 'steps', 'foods', 'interests', 'journey', 'exercises', 'book', 'aspects', 'boundaries', 'professionals', 'struggles', 'successes', 'books', 'places', 'strategies', 'answers', 'types', 'concepts', 'individuals', 'meditation', 'forums', 'studies', 'programs', 'link', 'ideas', 'questions', 'beliefs', 'strategy', 'tips', 'foundation', 'concept', 'substances', 'solutions', 'wisdom', 'method', 'philosophy', 'apps', 'outlets', 'forum', 'communities', 'topics', 'introspection', 'structure', 'outcomes', 'links', 'routines', 'info', 'remedies', 'data', 'qualities', 'accounts', 'treatments', 'stats', 'meditations', 'values', 'journeys', 'items', 'material', 'possibilities', 'recommendations', 'courses', 'services', 'cbt', 'elements', 'teachings', 'stoicism']\n",
        "progresswords = []\n",
        "\n",
        "for i in progwordbank:\n",
        "  if not i in progresswords:\n",
        "    progresswords.append(i);\n",
        "print (progresswords)\n",
        "\n",
        "fuckwordbank = ['frick', 'frickin', 'freakin', 'freaking,\"tweak\", \"tweaking\",\"fuck', 'fuckin', 'fucking', 'fucky', 'fucked, â€™fuckedness', 'motherfucking', 'motherfucker', 'damn', 'goddamn', 'shit', 'shitty', 'shittier', 'shittiest', 'blasted', 'bloody','gonna', 'haha', 'coke', 'had', 'yup', 'half', 'yeah', 'bad', 'depressed', 'hell', 'kinda', 'miserable', 'fucked', 'bitch', 'sucked', 'wtf', 'insane', 'ridiculous', 'fuck', 'drunk', 'awful', 'sucks', 'omg', 'ass', 'pathetic', 'nasty', 'tho', 'ha', 'oh', 'terrible', 'yesterday', 'sad', 'resin', 'dead', 'retarded', 'sack', 'everytime', 'stupid', 'lol', 'wicked', 'crazy', 'gross', 'crappy', 'fiend', 'sick', 'crack', 'grumpy', 'bc', 'cuz', 'nope', 'dope', 'yea', 'weird', 'horrible', 'lame', 'disgusting', 'dumb', 'balls', 'dank', 'hella', 'eh', 'didnt', 'crap', 'freaking', 'legit', 'yep', 'nah', 'dirty', 'fucker', 'pissed', 'junkie', 'garbage', 'hangovers', 'hahaha', 'dang', 'dying', 'bastard', 'burnt', 'cus', 'worthless', 'hahah', 'lmao', 'crackhead', 'af', 'bruh', 'carts','didn', 'last', 'shitty', 'anyways', 'mad', 'pussy', 'shit', 'fucking', 'alright', 'meh', 'hated', 'damn', 'lazy', 'straight', 'ok', 'moody', 'paranoid', 'tired', 'anxious', 'lethargic', 'goddamn', 'meth', 'irritable', 'sorta', 'dry', 'hungry', 'cos', 'nuts', 'embarrassing', 'cranky', 'annoying', 'broke', 'fuckin', 'scared', 'nauseated', 'bum', 'loser', 'tbh', 'guilty', 'till', 'boy', 'groggy', 'idiot', 'brutal', 'nauseous', 'hungover', 'foggy', 'bummed', 'wet', 'dizzy', 'terrified', 'rn', 'lmfao', 'bloody', 'fine', 'yup', 'okay', 'high', 'yeah', 'bad', 'stoned', 'afterwards', 'upset', 'depressed', 'blazed', 'kinda', 'weak', 'embarrassed', 'miserable', 'bored', 'fucked', 'bitch', 'sucked', 'wtf', 'insane', 'forgetful', 'stressed', 'awkward', 'ridiculous', 'drunk', 'awful', 'sucks', 'omg', 'boring', 'pathetic', 'terrible', 'yesterday', 'sad', 'empty', 'dead', 'retarded', 'stupid', 'angry', 'lol', 'nervous', 'crazy', 'gross', 'restless', 'crappy', 'fiend', 'sick', 'ashamed', 'grumpy', 'cuz', 'yea', 'weird', 'horrible', 'lame', 'baked', 'disgusting', 'dumb', 'depressing', 'exhausted', 'frustrated', 'irritated', 'disappointed', 'confused', 'bitter', 'cloudy', 'tempted', 'pissed', 'panicked', 'zombie', 'hazy', 'dehydrated', 'fatigued', 'uncomfortable', 'annoyed', 'crying', 'edgy', 'sore', 'agitated', 'dying', 'skinny', 'unmotivated', 'burnt', 'sluggish', 'hopeless', 'worthless', 'hahah', 'drained', 'lmao', 'sweaty', 'horny', 'bloated', 'af', 'odd', 'low', 'shitty', 'mad', 'pussy', 'fucking', 'hate', 'stuck', 'worse', 'alright', 'meh', 'sleepy', 'hated', 'lazy', 'ok', 'tempting', 'unhappy', 'unproductive', 'nasty', 'worried', 'overwhelmed', 'moody', 'paranoid', 'frustrating', 'tired', 'anxious', 'lethargic', 'dull', 'jealous', 'irritable', 'suicidal', 'dry', 'hungry', 'embarrassing', 'desperate', 'strange', 'cranky', 'annoying', 'fuckin', 'scared', 'nauseated', 'rough', 'bum', 'scary', 'loser', 'guilty', 'fried', 'groggy', 'antsy', 'trapped', 'apathetic', 'useless', 'nauseous', 'hungover', 'foggy', 'relieved', 'bummed', 'fuzzy', 'disgusted', 'pointless', 'unstable', 'unbearable', 'dizzy', 'terrified', 'thirsty', 'shaky', 'fine', 'okay', 'high', 'bad', 'stoned', 'upset', 'depressed', 'uneasy', 'normal', 'weak', 'embarrassed', 'miserable', 'bored', 'fucked', 'sucked', 'insane', 'forgetful', 'stressed', 'awkward', 'ridiculous', 'overwhelming', 'stressful', 'drunk', 'awful', 'sucks', 'boring', 'pathetic', 'terrible', 'sad', 'empty', 'retarded', 'stupid', 'angry', 'nervous', 'crazy', 'gross', 'restless', 'crappy', 'sick', 'numb', 'ashamed', 'grumpy', 'weird', 'horrible', 'lame', 'baked', 'disgusting', 'dumb', 'depressing', 'exhausted', 'frustrated', 'irritated', 'intense', 'disappointed', 'confused', 'bitter', 'insecure', 'cloudy', 'tempted', 'pissed', 'panicked', 'zombie', 'hazy', 'dehydrated', 'fatigued', 'uncomfortable', 'annoyed', 'edgy', 'drowsy', 'agitated', 'isolated', 'dying', 'unmotivated', 'burnt', 'sluggish', 'hopeless', 'worthless', 'drained', 'sweaty', 'horny', 'bloated', 'jittery', 'odd', 'low', 'shitty', 'afterwards', 'mad', 'blazed', 'hate', 'stuck', 'worse', 'alright', 'meh', 'sleepy', 'hated', 'lazy', 'ok', 'tempting', 'unhappy', 'unproductive', 'overwhelmed', 'moody', 'paranoid', 'unpleasant', 'frustrating', 'tired', 'anxious', 'lethargic', 'dull', 'irritable', 'suicidal', 'hungry', 'relaxed', 'embarrassing', 'desperate', 'impatient', 'exhausting', 'strange', 'cranky', 'annoying', 'scared', 'nauseated', 'rough', 'scary', 'guilty', 'fried', 'groggy', 'antsy', 'trapped', 'apathetic', 'useless', 'nauseous', 'hungover', 'foggy', 'defeated', 'sore', 'relieved', 'bummed', 'fuzzy', 'disgusted', 'pointless', 'unstable', 'unbearable', 'dizzy', 'terrified', 'disconnected', 'thirsty', 'shaky', 'gonna', 'haha', 'coke', 'had', 'yup', 'half', 'yeah', 'bad', 'depressed', 'hell', 'kinda', 'miserable', 'fucked', 'bitch', 'sucked', 'wtf', 'insane', 'ridiculous', 'fuck', 'drunk', 'awful', 'sucks', 'omg', 'ass', 'pathetic', 'nasty', 'tho', 'ha', 'oh', 'terrible', 'yesterday', 'sad', 'resin', 'dead', 'retarded', 'sack', 'everytime', 'stupid', 'lol', 'wicked', 'crazy', 'gross', 'crappy', 'fiend', 'sick', 'crack', 'grumpy', 'bc', 'cuz', 'nope', 'dope', 'yea', 'weird', 'horrible', 'lame', 'disgusting', 'dumb', 'balls', 'dank', 'hella', 'eh', 'didnt', 'crap', 'freaking', 'legit', 'yep', 'nah', 'dirty', 'fucker', 'pissed', 'junkie', 'garbage', 'hangovers', 'hahaha', 'dang', 'dying', 'bastard', 'burnt', 'cus', 'worthless', 'hahah', 'lmao', 'crackhead', 'af', 'bruh', 'carts', 'didn', 'last', 'shitty', 'anyways', 'mad', 'pussy', 'shit', 'fucking', 'alright', 'meh', 'hated', 'damn', 'lazy', 'straight', 'ok', 'moody', 'paranoid', 'tired', 'anxious', 'lethargic', 'goddamn', 'meth', 'irritable', 'sorta', 'dry', 'hungry', 'cos', 'nuts', 'embarrassing', 'cranky', 'annoying', 'broke', 'fuckin', 'scared', 'nauseated', 'bum', 'loser', 'tbh', 'guilty', 'till', 'boy', 'groggy', 'idiot', 'brutal', 'nauseous', 'hungover', 'foggy', 'bummed', 'wet', 'dizzy', 'terrified', 'rn', 'lmfao', 'bloody', 'fine', 'yup', 'okay', 'high', 'yeah', 'bad', 'stoned', 'afterwards', 'upset', 'depressed', 'blazed', 'kinda', 'weak', 'embarrassed', 'miserable', 'bored', 'fucked', 'bitch', 'sucked', 'wtf', 'insane', 'forgetful', 'stressed', 'awkward', 'ridiculous', 'drunk', 'awful', 'sucks', 'omg', 'boring', 'pathetic', 'terrible', 'yesterday', 'sad', 'empty', 'dead', 'retarded', 'stupid', 'angry', 'lol', 'nervous', 'crazy', 'gross', 'restless', 'crappy', 'fiend', 'sick', 'ashamed', 'grumpy', 'cuz', 'yea', 'weird', 'horrible', 'lame', 'baked', 'disgusting', 'dumb', 'depressing', 'exhausted', 'frustrated', 'irritated', 'disappointed', 'confused', 'bitter', 'cloudy', 'tempted', 'pissed', 'panicked', 'zombie', 'hazy', 'dehydrated', 'fatigued', 'uncomfortable', 'annoyed', 'crying', 'edgy', 'sore', 'agitated', 'dying', 'skinny', 'unmotivated', 'burnt', 'sluggish', 'hopeless', 'worthless', 'hahah', 'drained', 'lmao', 'sweaty', 'horny', 'bloated', 'af', 'odd', 'low', 'shitty', 'mad', 'pussy', 'fucking', 'hate', 'stuck', 'worse', 'alright', 'meh', 'sleepy', 'hated', 'lazy', 'ok', 'tempting', 'unhappy', 'unproductive', 'nasty', 'worried', 'overwhelmed', 'moody', 'paranoid', 'frustrating', 'tired', 'anxious', 'lethargic', 'dull', 'jealous', 'irritable', 'suicidal', 'dry', 'hungry', 'embarrassing', 'desperate', 'strange', 'cranky', 'annoying', 'fuckin', 'scared', 'nauseated', 'rough', 'bum', 'scary', 'loser', 'guilty', 'fried', 'groggy', 'antsy', 'trapped', 'apathetic', 'useless', 'nauseous', 'hungover', 'foggy', 'relieved', 'bummed', 'fuzzy', 'disgusted', 'pointless', 'unstable', 'unbearable', 'dizzy', 'terrified', 'thirsty', 'shaky', 'fine', 'okay', 'high', 'bad', 'stoned', 'upset', 'depressed', 'uneasy', 'normal', 'weak', 'embarrassed', 'miserable', 'bored', 'fucked', 'sucked', 'insane', 'forgetful', 'stressed', 'awkward', 'ridiculous', 'overwhelming', 'stressful', 'drunk', 'awful', 'sucks', 'boring', 'pathetic', 'terrible', 'sad', 'empty', 'retarded', 'stupid', 'angry', 'nervous', 'crazy', 'gross', 'restless', 'crappy', 'sick', 'numb', 'ashamed', 'grumpy', 'weird', 'horrible', 'lame', 'baked', 'disgusting', 'dumb', 'depressing', 'exhausted', 'frustrated', 'irritated', 'intense', 'disappointed', 'confused', 'bitter', 'insecure', 'cloudy', 'tempted', 'pissed', 'panicked', 'zombie', 'hazy', 'dehydrated', 'fatigued', 'uncomfortable', 'annoyed', 'edgy', 'drowsy', 'agitated', 'isolated', 'dying', 'unmotivated', 'burnt', 'sluggish', 'hopeless', 'worthless', 'drained', 'sweaty', 'horny', 'bloated', 'jittery', 'odd', 'low', 'shitty', 'afterwards', 'mad', 'blazed', 'hate', 'stuck', 'worse', 'alright', 'meh', 'sleepy', 'hated', 'lazy', 'ok', 'tempting', 'unhappy', 'unproductive', 'overwhelmed', 'moody', 'paranoid', 'unpleasant', 'frustrating', 'tired', 'anxious', 'lethargic', 'dull', 'irritable', 'suicidal', 'hungry', 'relaxed', 'embarrassing', 'desperate', 'impatient', 'exhausting', 'strange', 'cranky', 'annoying', 'scared', 'nauseated', 'rough', 'scary', 'guilty', 'fried', 'groggy', 'antsy', 'trapped', 'apathetic', 'useless', 'nauseous', 'hungover', 'foggy', 'defeated', 'sore', 'relieved', 'bummed', 'fuzzy', 'disgusted', 'pointless', 'unstable', 'unbearable', 'dizzy', 'terrified', 'disconnected', 'thirsty', 'shaky']\n",
        "fuckwords = []\n",
        "for i in fuckwordbank:\n",
        "  if not i in fuckwords:\n",
        "    fuckwords.append(i);\n",
        "print (fuckwords)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['discussion', 'find', 'understanding', 'useful', 'support', 'group', 'approach', 'subreddit', 'perspective', 'benefit', 'therapy', 'positive', 'improve', 'helpful', 'reflect', 'motivate', 'practice', 'sub', 'focused', 'provide', 'information', 'knowledge', 'encourage', 'meditate', 'focusing', 'mindfulness', 'guidance', 'outlook', 'skill', 'guide', 'headspace', 'technique', 'forum', 'compassion', 'practicing', 'tool', 'insight', 'techniques', 'achieve', 'accountability', 'awareness', 'meditating', 'buddhism', 'channel', 'importance', 'identify', 'beneficial', 'engage', 'explore', 'constructive', 'mindful', 'cultivate', 'resources', 'resource', 'acceptance', 'journaling', 'grounded', 'positivity', 'spirituality', 'mediation', 'gratitude', 'refocus', 'community', 'help', 'exercise', 'success', 'create', 'recovery', 'others', 'address', 'methods', 'learn', 'interests', 'journey', 'seek', 'practical', 'strategies', 'reflection', 'meditation', 'develop', 'discipline', 'value', 'growth', 'ideas', 'focus', 'strategy', 'tips', 'behavioral', 'solutions', 'wisdom', 'philosophy', 'introspection', 'structure', 'kindness', 'implement', 'info', 'discuss', 'utilize', 'strengthen', 'dbt', 'meditations', 'reinforce', 'cbt', 'stoicism', 'build', 'experiences', 'form', 'overcome', 'ways', 'suggestions', 'activities', 'groups', 'research', 'insights', 'website', 'lessons', 'psychology', 'mechanisms', 'alternatives', 'literature', 'sources', 'principles', 'behaviors', 'tools', 'perspectives', 'practices', 'habits', 'areas', 'network', 'subreddits', 'challenges', 'options', 'evidence', 'program', 'advice', 'hobbies', 'strength', 'exercises', 'aspects', 'boundaries', 'books', 'concepts', 'studies', 'programs', 'link', 'beliefs', 'foundation', 'concept', 'method', 'apps', 'outlets', 'communities', 'topics', 'links', 'routines', 'data', 'qualities', 'values', 'material', 'recommendations', 'people', 'meetings', 'points', 'things', 'medications', 'stories', 'posts', 'websites', 'site', 'folks', 'projects', 'goals', 'addictions', 'path', 'results', 'vices', 'diet', 'ones', 'threads', 'articles', 'subs', 'skills', 'circumstances', 'examples', 'efforts', 'parts', 'opinions', 'rules', 'supplements', 'factors', 'videos', 'distractions', 'conditions', 'forms', 'guidelines', 'therapists', 'traits', 'affirmations', 'friendships', 'context', 'reasons', 'quotes', 'stuff', 'words', 'benefits', 'steps', 'foods', 'book', 'professionals', 'struggles', 'successes', 'places', 'answers', 'types', 'individuals', 'forums', 'questions', 'substances', 'outcomes', 'remedies', 'accounts', 'treatments', 'stats', 'journeys', 'items', 'possibilities', 'courses', 'services', 'elements', 'teachings']\n",
            "['frick', 'frickin', 'freakin', 'freaking,\"tweak\", \"tweaking\",\"fuck', 'fuckin', 'fucking', 'fucky', 'fucked, â€™fuckedness', 'motherfucking', 'motherfucker', 'damn', 'goddamn', 'shit', 'shitty', 'shittier', 'shittiest', 'blasted', 'bloody', 'gonna', 'haha', 'coke', 'had', 'yup', 'half', 'yeah', 'bad', 'depressed', 'hell', 'kinda', 'miserable', 'fucked', 'bitch', 'sucked', 'wtf', 'insane', 'ridiculous', 'fuck', 'drunk', 'awful', 'sucks', 'omg', 'ass', 'pathetic', 'nasty', 'tho', 'ha', 'oh', 'terrible', 'yesterday', 'sad', 'resin', 'dead', 'retarded', 'sack', 'everytime', 'stupid', 'lol', 'wicked', 'crazy', 'gross', 'crappy', 'fiend', 'sick', 'crack', 'grumpy', 'bc', 'cuz', 'nope', 'dope', 'yea', 'weird', 'horrible', 'lame', 'disgusting', 'dumb', 'balls', 'dank', 'hella', 'eh', 'didnt', 'crap', 'freaking', 'legit', 'yep', 'nah', 'dirty', 'fucker', 'pissed', 'junkie', 'garbage', 'hangovers', 'hahaha', 'dang', 'dying', 'bastard', 'burnt', 'cus', 'worthless', 'hahah', 'lmao', 'crackhead', 'af', 'bruh', 'carts', 'didn', 'last', 'anyways', 'mad', 'pussy', 'alright', 'meh', 'hated', 'lazy', 'straight', 'ok', 'moody', 'paranoid', 'tired', 'anxious', 'lethargic', 'meth', 'irritable', 'sorta', 'dry', 'hungry', 'cos', 'nuts', 'embarrassing', 'cranky', 'annoying', 'broke', 'scared', 'nauseated', 'bum', 'loser', 'tbh', 'guilty', 'till', 'boy', 'groggy', 'idiot', 'brutal', 'nauseous', 'hungover', 'foggy', 'bummed', 'wet', 'dizzy', 'terrified', 'rn', 'lmfao', 'fine', 'okay', 'high', 'stoned', 'afterwards', 'upset', 'blazed', 'weak', 'embarrassed', 'bored', 'forgetful', 'stressed', 'awkward', 'boring', 'empty', 'angry', 'nervous', 'restless', 'ashamed', 'baked', 'depressing', 'exhausted', 'frustrated', 'irritated', 'disappointed', 'confused', 'bitter', 'cloudy', 'tempted', 'panicked', 'zombie', 'hazy', 'dehydrated', 'fatigued', 'uncomfortable', 'annoyed', 'crying', 'edgy', 'sore', 'agitated', 'skinny', 'unmotivated', 'sluggish', 'hopeless', 'drained', 'sweaty', 'horny', 'bloated', 'odd', 'low', 'hate', 'stuck', 'worse', 'sleepy', 'tempting', 'unhappy', 'unproductive', 'worried', 'overwhelmed', 'frustrating', 'dull', 'jealous', 'suicidal', 'desperate', 'strange', 'rough', 'scary', 'fried', 'antsy', 'trapped', 'apathetic', 'useless', 'relieved', 'fuzzy', 'disgusted', 'pointless', 'unstable', 'unbearable', 'thirsty', 'shaky', 'uneasy', 'normal', 'overwhelming', 'stressful', 'numb', 'intense', 'insecure', 'drowsy', 'isolated', 'jittery', 'unpleasant', 'relaxed', 'impatient', 'exhausting', 'defeated', 'disconnected']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oorXf3kkd50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "6896aaa5-7809-4352-89d0-f9160bb5b813"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-48a22d34f1ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mcycle1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cycle1' is not defined"
          ]
        }
      ]
    }
  ]
}